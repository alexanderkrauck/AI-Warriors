verbose: 1
mop:
  # set to false when preprocessing the test set, then the manifest and other stuff is used differently
  train_set: true
  has_labels: true
  # if true the features will not be recomputed from scratch
  additive_preprocessing: false
  preprocessing_manifest: "./prepro_manifest.yaml"
  # directory where the statistics necessary to compute everything are placed
  prepro_statistics_dir: "feature_stats_training"
  # directory where the features will be dumped
  feature_dir: "preprocessed_training_features"
  # directories for the corresponding states of the original data
  preprocessed_dir: "preprocessed_training_files"
  compressed_dir: "compressed_data"
  uncompressed_dir: "training_files"
  # either 'comp', 'uncomp' or 'parquet', if parquet is used, stuff's gonna get appended to whatever is already there
  load_from: "uncomp"
  manifesto: "generated_manifest.yaml"
compute:
  # bump this thing up when we are on the cluster - it introduces overhead
  chunksize: "128MB"
  # by balancing these three we can adjust for the capacities of a given machine (6 core/ 16 GB ram in my case)
  # actually quite a tricky thing to do, rule of thumb: more workers = more memory,
  # more threads per worker - more memory actually used by the worker
  # more memory - bigger toaster needed to run it
  # also more memory per worker and thread makes the computation more reliable
  # there is a scenario where the computation stops because the workers keep running out of memory and diying/idling.
  # I recommend at least 2GB per thread for reliable operation.
  n_workers: 32
  n_threads_per_worker: 2
  mem_lim: "6GB"

features:
  # basic features from the list to compute (computed in this order)
  basic_features:
    - "has_reply"
    - "bert_token_len"
    - "has_retweet"
    - "has_retweet_comment"
    - "has_like"
    - "n_photos"
    - "n_videos"
    - "n_gifs"
    - "type_encoding"
    - "language_encoding"
    - "a_followers"
    - "a_following"
    - "b_followers"
    - "b_following"
    - "day_of_week"
    - "hour_of_day"
    - "b_creation_delta"
    - "a_creation_delta"
    - "tweet_hash"
    - "b_hash"
    - "a_hash"
  keep_features:
    - "b_is_verified"
    - "a_is_verified"
    - "a_follows_b"
    - "b_user_id"
    - "tweet_id"


  # target encodings to compute (to be computed as per the nvidia paper) (this one is just an example)
  TE_smoothing_factor: 20
  TE_features:
    language_encoding:
      - "has_reply"
      - "has_like"
      - "has_retweet_comment"
      - "has_retweet"
    type_encoding:
      - "has_reply"
      - "has_like"
      - "has_retweet_comment"
      - "has_retweet"
  # categorical columns to compute the marginal probabilities for
  marginal_prob_columns:
    dump_to: "naive_features.csv"
    features:
      - "a_follows_b"
      - "b_is_verified"
      - "a_is_verified"
      - "type"
      - "language"
    per_labels:
      - "has_reply"
      - "has_retweet"
      - "has_retweet_comment"
      - "has_like"
