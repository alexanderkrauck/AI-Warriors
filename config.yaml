verbose: 2
# bump this thing up when we are on the cluster - it introduces overhead
chunksize: "128MB"
# by balancing these three we can adjust for the capacities of a given machine (6 core/ 16 GB ram in my case)
# actually quite a tricky thing to do, rule of thumb: more workers = more memory,
# more threads per worker - more memory actually used by the worker
# more memory - bigger toaster needed to run it
n_workers: 4
n_threads_per_worker: 2
mem_lim: "3GB"
# either 'comp' or 'parquet'
# if parquet is used, stuff's gonna get appended to whatever is already there
load_from: "parquet"
# categorical columns to compute the marginal probabilities for
marginal_prob_columns:
  - ""
# numerical columns to compute mean and std for
mean_std_columns:
  - ""
# basic features from the list to compute
basic_features:
  - "has_reply"
# target encodings to compute (as per the nvidia paper)
TE_features: {language: "has_reply"}
